#Visualize the relationship between k and Kappa with a scatter plot.
Kappa_dataframe <- as.data.frame(cbind(k = seq(1:100), Kappa_knn))
Kappa_vs_k <-
ggplot(Kappa_dataframe, aes(x = k, y = Kappa_knn)) + geom_point()
Kappa_vs_k
#Select the best model and Kappa for the training set.
set.seed(1)
nn_best_train <-
kNN(
Decision ~ .,
as.matrix(train_data_numeric),
as.matrix(train_data_numeric),
k = which.max(Kappa_knn)
)
nn_best_conting_train <-
table(nn_best_train,
train_data_numeric$Decision,
dnn = c("Predicted", "Actual"))
nn_best_cm_train <- confusionMatrix(nn_best_conting_train)
Kappa_train_bestKNN <- nn_best_cm_train$overall["Kappa"]
Kappa_train_bestKNN
#Select the best model and Kappa for the test set.
set.seed(1)
nn_best_test <-
kNN(
Decision ~ .,
as.matrix(train_data_numeric),
as.matrix(test_data_numeric),
k = which.max(Kappa_knn)
)
nn_best_conting_test <-
table(nn_best_test,
test_data_numeric$Decision,
dnn = c("Predicted", "Actual"))
nn_best_cm_test <- confusionMatrix(nn_best_conting_test)
Kappa_test_bestKNN <- nn_best_cm_test$overall["Kappa"]
Kappa_test_bestKNN
kappa_model_df$kappa_scores[kappa_model_df$model_name == "KNN"] <-
Kappa_test_bestKNN
## Kappa Score: 0.378012
### Simple Classification Tree ###
# Fit a simple classification tree using the "tree" function, with the "gini" splitting criterion
set.seed(1)
simple_tree <- tree(Decision ~ ., split = "gini", train_data_2)
# Print a summary of the tree, including the number of internal nodes, the number of terminal nodes, and classification accuracy on the training set
summary(simple_tree)
# Graphically display the tree using the "plot" and "text" functions, with "pretty=0" to include category names for qualitative predictors
plot(simple_tree)
text(simple_tree, pretty = 0)
# Calculate the classification performance (Kappa) on the training set by predicting class labels
# for each observation and comparing to the actual class labels
simple_tree_pred_train <-
predict(simple_tree, train_data_2, type = "class")
simple_tree_conting_train <-
table(simple_tree_pred_train,
train_data_2$Decision,
dnn = c("Predicted", "Actual"))
simple_tree_cm_train <- confusionMatrix(simple_tree_conting_train)
simple_tree_cm_train$overall["Kappa"]
# Kappa = 0.6909397, indicating the simple tree strongly agrees with reality in the training set
# Calculate the classification performance (Kappa) on the test set by predicting class labels for each observation and comparing to the actual class labels
simple_tree_pred_test <-
predict(simple_tree, test_data_2, type = "class")
simple_tree_conting_test <-
table(simple_tree_pred_test,
test_data_2$Decision,
dnn = c("Predicted", "Actual"))
simple_tree_cm_test <- confusionMatrix(simple_tree_conting_test)
simple_tree_cm_test$overall["Kappa"]
kappa_model_df$kappa_scores[kappa_model_df$model_name == "Simple Clas. Tree"] <-
simple_tree_cm_test$overall["Kappa"]
## Kappa Score: 0.4208681
### Tree Pruning ###
set.seed(1)
# Perform 10-fold cross-validation to determine the optimal number of terminal nodes
cv_Tree <- cv.tree(simple_tree, FUN = prune.misclass, K = 10)
# Identify the optimal number of terminal nodes
prune_size <- cv_Tree$size[which.min(cv_Tree$dev)]
plot(cv_Tree$size, cv_Tree$dev, type = "b")
# Prune the tree to obtain the tree with the optimal number of terminal nodes
prune_tree <- prune.misclass(simple_tree, best = prune_size)
# Display the pruned tree
plot(prune_tree)
text(prune_tree, pretty = 0)
# Evaluate the classification performance on the training set
prune_tree_pred_train <-
predict(prune_tree, train_data_2, type = "class")
prune_tree_conting_train <-
table(prune_tree_pred_train,
train_data_2$Decision,
dnn = c("Predicted", "Actual"))
prune_tree_cm_train <- confusionMatrix(prune_tree_conting_train)
prune_tree_cm_train$overall["Kappa"]
#Kappa: 0.6906284 indicating the pruned tree strongly agrees with reality
#in the training set.
# Evaluate the classification performance on the test set
prune_tree_pred_test <-
predict(prune_tree, test_data_2, type = "class")
prune_tree_conting_test <-
table(prune_tree_pred_test,
test_data_2$Decision,
dnn = c("Predicted", "Actual"))
prune_tree_cm_test <- confusionMatrix(prune_tree_conting_test)
prune_tree_cm_test$overall["Kappa"]
kappa_model_df$kappa_scores[kappa_model_df$model_name == "Tree Pruning"] <-
prune_tree_cm_test$overall["Kappa"]
## Kappa score: 0.4163069
### Bagging ###
# Create 500 bootstrap datasets to perform bagging
# Set seed to make the random sampling with replacement reproducible
set.seed(1)
# Perform tree bagging using randomForest function
Tree_Bagging <- randomForest(
Decision ~ .,
data = train_data_2,
ntrees = 500,
mtry = 19,
split = "gini",
replace = TRUE,
importance = TRUE
)
# View Tree_Bagging object
Tree_Bagging
# Plot OOB classification error for each individual class and all classes combined
plot(Tree_Bagging)
# View OOB classification errors for High, Low, and Both combined
Tree_Bagging$err.rate
# Find the optimal ntrees
which.min(Tree_Bagging$err.rate[, 1])
# View overall summary of the importance of each predictor using importance()
importance(Tree_Bagging)
# Make a plot to better visualize the importance of each variable
varImpPlot(Tree_Bagging)
# View classification performance (Kappa) for the training set
bag_train_pred <-
predict(Tree_Bagging, train_data_2, type = "class")
bag_conting_train <-
table(bag_train_pred,
train_data_2$Decision,
dnn = c("Predicted", "Actual"))
bag_cm_train <- confusionMatrix(bag_conting_train)
bag_cm_train$overall["Kappa"]
#Kappa = 1, indicating the bagged tree very strongly
#agrees with reality in training set.
# View classification performance (Kappa) for the test set
bag_test_pred <- predict(Tree_Bagging, test_data_2, type = "class")
bag_conting_test <-
table(bag_test_pred,
test_data_2$Decision,
dnn = c("Predicted", "Actual"))
bag_cm_test <- confusionMatrix(bag_conting_test)
bag_cm_test$overall["Kappa"]
kappa_model_df$kappa_scores[kappa_model_df$model_name == "Class. Tree + Bagging"] <-
bag_cm_test$overall["Kappa"]
## Kappa score: 0.4954846
### Random Forest ###
#Perform RF using 500 bootstrap data sets and optimal mtry
#RF uses a subset of predictors for each split of a tree, so optimal mtry can be determined by testing different values
# Loop to find optimal mtry value for Random Forest
Test_Kappa_RF <- rep(0, 20)
for (i in 1:19) {
set.seed(1)
Tree_RF <- randomForest(
Decision ~ .,
data = train_data_2,
ntrees = 500,
mtry = i,
split = "gini",
replace = TRUE,
importance = TRUE
)
Test_pred_RF <- predict(Tree_RF, test_data_2, type = "class")
RF_conting_test <- table(Test_pred_RF,
test_data_2$Decision,
dnn = c("Predicted", "Actual"))
RF_cm_test <- confusionMatrix(RF_conting_test)
Test_Kappa_RF[i] <- RF_cm_test$overall["Kappa"]
}
Test_Kappa_RF
# Find the index of the highest Kappa value
which.max(Test_Kappa_RF)
# Output the mtry value with the highest Kappa value
# mtry =  4 gives the highest Kappa.
Test_Kappa_RF[which.max(Test_Kappa_RF)]
Tree_RF <- randomForest(
Decision ~ .,
data = train_data_2,
ntrees = 500,
mtry = 4,
split = "gini",
replace = TRUE,
importance = TRUE
)
#Classification performance (Kappa) for the training set
rf_train_pred <- predict(Tree_RF, train_data_2, type = "class")
rf_conting_train <- table(rf_train_pred,
train_data_2$Decision,
dnn = c("Predicted", "Actual"))
rf_cm_train <- confusionMatrix(rf_conting_train)
rf_cm_train$overall["Kappa"]
#Kappa = 0.9937597, indicating very strong agreement with reality in the training set.
#Classification performance (Kappa) for the test set
rf_test_pred <- predict(Tree_RF, test_data_2, type = "class")
rf_conting_test <- table(rf_test_pred,
test_data_2$Decision,
dnn = c("Predicted", "Actual"))
rf_cm_test <- confusionMatrix(rf_conting_test)
rf_cm_test$overall["Kappa"]
kappa_model_df$kappa_scores[kappa_model_df$model_name == "Class. Trees + RF"] <-
rf_cm_test$overall["Kappa"]
## Kappa Score: 0.5086026
### Boosting ###
#Use gbm() to build a binary classification model since the response has two
#classes, with distribution = "bernoulli". Use a loop to find the optimal
#n.trees for each interaction.depth value, then use the obtained optimal values
#to fit the model.
train_data_3 <- train_data_2
test_data_3 <- test_data_2
train_data_3$Decision <- as.numeric(train_data_3$Decision)
train_data_3$Decision <- ifelse(train_data_3$Decision == 2, 1, 0)
test_data_3$Decision <- as.numeric(test_data_3$Decision)
test_data_3$Decision <- ifelse(test_data_3$Decision == 2, 1, 0)
n_trees <- rep(0, 6)
min_cv_error <- rep(0, 6)
for (i in 1:6) {
set.seed(1)
Tree_Boosting <-
gbm(
Decision ~ .,
data = train_data_3,
distribution = "bernoulli",
n.trees = 1500,
interaction.depth = i,
cv.folds = 10,
shrinkage = 0.01
)
n_trees[i] <- which.min(Tree_Boosting$cv.error)
min_cv_error[i] <-
Tree_Boosting$cv.error[which.min(Tree_Boosting$cv.error)]
}
#Find which interaction.depth value gives the lowest cv error and its
#corresponding n.trees.
which.min(min_cv_error)
n_trees[5]
#Set the optimal n.trees = 1454 and interaction.depth = 5 from the loop and use them to fit
#the model.
set.seed(1)
Tree_Boosting <- gbm(
Decision ~ .,
data = train_data_3,
distribution = "bernoulli",
n.trees = 1454,
interaction.depth = 5,
shrinkage = 0.01
)
#Print a relative influence plot and the relative influence statistics of each variable in the model.
summary(Tree_Boosting)
#Use the model to predict the probability of Y=1 for each training observation and assign each predicted probability to a class label.
boost_prob_train <-
predict(Tree_Boosting, type = "response", train_data_3)
boost_pred_results_train <- ifelse(boost_prob_train > 0.5, 1, 0)
#Compute the confusion matrix and Kappa value for the training set.
boost_conting_train <-
table(boost_pred_results_train,
train_data_3$Decision,
dnn = c("Predicted", "Actual"))
boost_cm_train <- confusionMatrix(boost_conting_train)
boost_cm_train$overall["Kappa"]
#Use the model to predict the probability of Y=1 for each test observation and assign each predicted probability to a class label.
boost_prob_test <-
predict(Tree_Boosting, type = "response", test_data_3)
boost_pred_results_test <- ifelse(boost_prob_test > 0.5, 1, 0)
#Compute the confusion matrix and Kappa value for the test set.
boost_conting_test <-
table(boost_pred_results_test,
test_data_3$Decision,
dnn = c("Predicted", "Actual"))
boost_cm_test <- confusionMatrix(boost_conting_test)
boost_cm_test$overall["Kappa"]
# kappa: 0.503336
#A loop to determine optimal cut-off probability
Kappa <- rep(0, 36)
cutoff_prob <- rep(0, 36)
index <- 0
for (i in seq(from = 0.2, to = 0.9, by = 0.02)) {
index <- index + 1
boost_pred_results_test <- ifelse(boost_prob_test > i, 1, 0)
boost_conting_test <-
table(boost_pred_results_test,
test_data_3$Decision,
dnn = c("Predicted", "Actual"))
boost_cm_test <- confusionMatrix(boost_conting_test)
cutoff_prob[index] <- i
Kappa[index] <- boost_cm_test$overall["Kappa"]
}
which.max(Kappa)
cutoff_prob[which.max(Kappa)]
Kappa[which.max(Kappa)]
# best cutoff prob: 0.38
#recalculate training kappa
boost_prob_train <-
predict(Tree_Boosting, type = "response", train_data_3)
boost_pred_results_train <-
ifelse(boost_prob_train > cutoff_prob[which.max(Kappa)], 1, 0)
#Compute the confusion matrix and Kappa value for the training set.
boost_conting_train <-
table(boost_pred_results_train,
train_data_3$Decision,
dnn = c("Predicted", "Actual"))
boost_cm_train <- confusionMatrix(boost_conting_train)
boost_cm_train$overall["Kappa"]
#recalulate test kappa
boost_prob_test <-
predict(Tree_Boosting, type = "response", test_data_3)
boost_pred_results_test <-
ifelse(boost_prob_test > cutoff_prob[which.max(Kappa)], 1, 0)
#Compute the confusion matrix and Kappa value for the test set.
boost_conting_test <-
table(boost_pred_results_test,
test_data_3$Decision,
dnn = c("Predicted", "Actual"))
boost_cm_test <- confusionMatrix(boost_conting_test)
boost_cm_test$overall["Kappa"]
kappa_model_df$kappa_scores[kappa_model_df$model_name == "Class. Trees + Boosting"] <-
boost_cm_test$overall["Kappa"]
## Kappa Score: 0.5366825
###SVM with linear Kernel###
# Fit support vector classifier with linear kernel
svm_linear <-
svm(
Decision ~ .,
data = test_data_2,
type = "C-classification",
kernel = "linear",
scale = TRUE,
cost = 1
)
# Print model summary
summary(svm_linear)
# Identify support vectors
svm_linear$index
# Compute the coefficients of the SVM model
coefficients <- t(svm_linear$SV) %*% svm_linear$coefs
intercept <- -svm_linear$rho
# Identify top 5 most important predictors
coefficients_abs <- abs(coefficients)
top_5_predictors <-
names(sort(coefficients_abs[, 1], decreasing = TRUE))[1:5]
top_5_predictors
## "AthleteAthlete..Opt.Out"            "LegacyLegacy..Opt.Out"              "Total.Event.Participation2.or.more"
## "Decision.PlanEarly.Decision.I"      "Decision.PlanEarly.Decision.II"
# Calculate training Kappa
svc_pred_train <- predict(svm_linear, train_data_2)
svc_conting_train <- table(svc_pred_train,
train_data_2$Decision,
dnn = c("Predicted", "Actual"))
svc_confu_train <- confusionMatrix(svc_conting_train)
svc_confu_train$overall["Kappa"]
# Calculate test kappa
svc_pred_test <- predict(svm_linear, test_data_2)
svc_conting_test <- table(svc_pred_test,
test_data_2$Decision,
dnn = c("Predicted", "Actual"))
svc_conting_test
svc_confu_test <- confusionMatrix(svc_conting_test)
svc_confu_test$overall["Kappa"]
# Kappa score: 0.4694264
# Coarse tuning
set.seed(1)
# for full cost range, it crashes, use 2^3 as tested by Dr. Zhu
svc_best_cost <-
svm(Decision ~ .,
data = train_data_2,
kernel = "linear",
cost = 2 ^ 3)
summary(svc_best_cost)
#coefficient and intercept
coefficients_best_svm_linear <-
t(svc_best_cost$SV) %*% svc_best_cost$coefs
Intercept_best_svm_linear <- -svc_best_cost$rho
#5 most important features
coeff_names <- rownames(coefficients_best_svm_linear)
coeff_abs <- abs(coefficients_best_svm_linear)
coeff_data <- data.frame(coeff_names, coeff_abs, row.names = NULL)
coeff_data[with(coeff_data, order(-coeff_abs)),][1:5,]
# top 5 predictors
# coeff_names
# LegacyLegacy..Opt.Out
# AthleteAthlete..Opt.Out
# Decision.PlanEarly.Decision.II
# Decision.PlanEarly.Decision.I
# Total.Event.Participation2.or.more
#train kappa
svc_pred_train_best <- predict(svc_best_cost, train_data_2)
svc_conting_train_best <-
table(svc_pred_train_best,
train_data_2$Decision,
dnn = c("Predicted", "Actual"))
svc_confu_train_best <- confusionMatrix(svc_conting_train_best)
svc_confu_train_best$overall["Kappa"]
# kappa: 0.4647546
#test kappa
svc_pred_test_best <- predict(svc_best_cost, test_data_2)
svc_conting_test_best <-
table(svc_pred_test_best,
test_data_2$Decision,
dnn = c("Predicted", "Actual"))
svc_confu_test_best <- confusionMatrix(svc_conting_test_best)
svc_confu_test_best$overall["Kappa"]
# kappa: 0.4531047
kappa_model_df$kappa_scores[kappa_model_df$model_name == "SVM + Linear. kernel"] <-
svc_confu_test_best$overall["Kappa"]
## SVM with Polynominal Kernel ##
set.seed(1)
# for full cost range it crashes, use parameters as tested by Dr. Zhu
svc_best_cost_poly <-
svm(
Decision ~ .,
data = train_data_2,
kernel = "polynomial",
cost = 2 ^ 7,
degree = 2
)
summary(svc_best_cost_poly)
#coefficient and intercept
coefficients_best_svm_poly <-
t(svc_best_cost_poly$SV) %*% svc_best_cost_poly$coefs
Intercept_best_svm_poly <- -svc_best_cost_poly$rho
#5 most important features
coeff_names <- rownames(coefficients_best_svm_poly)
coeff_abs <- abs(coefficients_best_svm_poly)
coeff_data <- data.frame(coeff_names, coeff_abs, row.names = NULL)
coeff_data[with(coeff_data, order(-coeff_abs)),][1:5,]
# coeff_names
# 46         Total.Event.Participation1
# 57      Academic_Performance
# 25            AthleteAthlete..Opt.Out
# 47 Total.Event.Participation2.or.more
# 23              LegacyLegacy..Opt.Out
#train kappa
svmp_pred_train_best <- predict(svc_best_cost_poly, train_data_2)
svmp_conting_train_best <-
table(svmp_pred_train_best,
train_data_2$Decision,
dnn = c("Predicted", "Actual"))
svmp_conting_train_best
svmp_confu_train_best <- confusionMatrix(svmp_conting_train_best)
svmp_confu_train_best$overall["Kappa"]
# kappa: 0.5673587
#test kappa
svmp_pred_test_best <- predict(svc_best_cost_poly, test_data_2)
svmp_conting_test_best <-
table(svmp_pred_test_best,
test_data_2$Decision,
dnn = c("Predicted", "Actual"))
svmp_conting_test_best
svmp_confu_test_best <- confusionMatrix(svmp_conting_test_best)
svmp_confu_test_best$overall["Kappa"]
# kappa: 0.4741636
kappa_model_df$kappa_scores[kappa_model_df$model_name == "SVM + Poly. kernel"] <-
svmp_confu_test_best$overall["Kappa"]
## SVM with Radial Kernel ##
# Fit support vector classifier with radial kernel
# Parameters given by Dr. Zhu
set.seed(1)
svm_radial <- svm(
Decision ~ .,
kernel = "radial",
data = train_data_2,
cost = 2 ^ 4,
gamma = 2 ^ -4,
scale = TRUE
)
# Print model summary
summary(svm_radial)
# Identify support vectors
svm_radial$index
# Calculate training Kappa
svmr_pred_train <- predict(svm_radial, train_data_2)
svmr_conting_train <-
table(svmr_pred_train,
train_data_2$Decision,
dnn = c("Predicted", "Actual"))
svmr_conting_train
svmr_confu_train <- confusionMatrix(svmr_conting_train)
svmr_confu_train$overall["Kappa"]
# kappa: 0.7616075
# Calculate test kappa
svmr_pred_test <- predict(svm_radial, test_data_2)
svmr_conting_test <- table(svmr_pred_test,
test_data_2$Decision,
dnn = c("Predicted", "Actual"))
svmr_conting_test
svmr_confu_test <- confusionMatrix(svmr_conting_test)
svmr_confu_test$overall["Kappa"]
# Kappa score: 0.4866242
kappa_model_df$kappa_scores[kappa_model_df$model_name == "SVM + Radial kernel"] <-
svmr_confu_test$overall["Kappa"]
## Analysis of the best model (logistic regression) ##
# Run model summary again
summary(best_logistic_model)
# Check coefficient plots
coefplot(best_logistic_model)
logistic_analysis <- coefplot(best_logistic_model, plot = FALSE)
analysis_df <- data.frame(predictor = logistic_analysis_df$Coefficient, value = logistic_analysis_df$Value)
analysis_df <- analysis_df[order(abs(analysis_df$value), decreasing = TRUE), ]
view(analysis_df[1:10,])
## Time Analysis ##
end_time <- Sys.time()
elapsed_time <- end_time - start_time
cat(paste0("Elapsed time: ", format(round(elapsed_time, 1), units = "secs")))
view(analysis_df[1:10,])
